{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "th.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataloaders**\n",
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.Subset'>\n",
      "train: 3872\n",
      "val: 484\n",
      "test: 484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BenaventeD\\anaconda3\\envs\\project_smad\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch as th\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "dataset_path = 'C:\\\\Users\\\\BenaventeD\\\\data\\\\s-mad-dataset\\\\preprocessed-frgc\\\\dataset_test\\\\'\n",
    "\n",
    "transf = v2.Compose([\n",
    "                        v2.Resize(size=(224,224)),\n",
    "                        v2.RandomHorizontalFlip(p=.5),\n",
    "                        v2.ToTensor(),\n",
    "                        # transforms.PILToTensor()\n",
    "                        ])\n",
    "\n",
    "train_data = ImageFolder(root = dataset_path, transform = transf, \n",
    "                #    is_valid_file = checkImage\n",
    "                   )\n",
    "\n",
    "generator = th.Generator().manual_seed(42)\n",
    "train_data, val_data = th.utils.data.random_split(dataset=train_data, lengths=[.8,.2], generator=generator)\n",
    "val_data, test_data = th.utils.data.random_split(dataset=val_data, lengths=[.5,.5], generator=generator)\n",
    "\n",
    "print(type(train_data))\n",
    "print('train:',len(train_data))\n",
    "print('val:',len(val_data))\n",
    "print('test:',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dim: torch.Size([32, 3, 224, 224]) torch.Size([32])\n",
      "val dim: torch.Size([32, 3, 224, 224]) torch.Size([32])\n",
      "test dim: torch.Size([32, 3, 224, 224]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "\n",
    "x,y = next(iter(train_loader))\n",
    "print('train dim:', x.size(), y.size())\n",
    "\n",
    "x,y = next(iter(val_loader))\n",
    "print('val dim:', x.size(), y.size())\n",
    "\n",
    "x,y = next(iter(test_loader))\n",
    "print('test dim:', x.size(), y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing pretrained models**\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ResNet18**\n",
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torchvision as tv\n",
    "small_resnet = tv.models.resnet18(pretrained=True)\n",
    "img = th.rand((1,3,224,224))\n",
    "x = small_resnet(img)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgpool layer Identity()\n",
      "fc layer Identity()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25088])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_resnet.avgpool = nn.Identity()\n",
    "small_resnet.fc = nn.Identity()\n",
    "print('avgpool layer',small_resnet.avgpool)\n",
    "print('fc layer',small_resnet.fc)\n",
    "img = th.rand((1,3,224,224))\n",
    "x = small_resnet(img)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): Identity()\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **EfficientNetB3**\n",
    "_______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "efficientnetb3 = tv.models.efficientnet_b3(weights='IMAGENET1K_V1')\n",
    "img = th.rand((1,3,224,224))\n",
    "x = efficientnetb3(img)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.3, inplace=True)\n",
       "  (1): Linear(in_features=1536, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnetb3.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgpool layer Identity()\n",
      "fc layer Identity()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 75264])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnetb3.avgpool = nn.Identity()\n",
    "efficientnetb3.classifier = nn.Identity()\n",
    "print('avgpool layer',efficientnetb3.avgpool)\n",
    "print('fc layer',efficientnetb3.classifier)\n",
    "img = th.rand((1,3,224,224))\n",
    "x = efficientnetb3(img)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.007692307692307693, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.015384615384615385, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.02307692307692308, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03076923076923077, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.038461538461538464, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.04615384615384616, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05384615384615385, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06153846153846154, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06923076923076923, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07692307692307693, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08461538461538462, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09230769230769233, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1076923076923077, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.11538461538461539, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.12307692307692308, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.13076923076923078, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=816, bias=False)\n",
       "            (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.13846153846153847, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14615384615384616, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15384615384615385, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.16153846153846155, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.16923076923076924, mode=row)\n",
       "      )\n",
       "      (5): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17692307692307693, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)\n",
       "            (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.18461538461538465, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)\n",
       "            (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.19230769230769232, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): Identity()\n",
       "  (classifier): Identity()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnetb3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **MobileNetV3**\n",
    "__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "mobilenetv3_large = tv.models.mobilenet_v3_large(weights='IMAGENET1K_V1')\n",
    "img = th.rand((1,3,224,224))\n",
    "x = mobilenetv3_large(img)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=960, out_features=1280, bias=True)\n",
       "  (1): Hardswish()\n",
       "  (2): Dropout(p=0.2, inplace=True)\n",
       "  (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenetv3_large.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=47040, out_features=3200, bias=True)\n",
      "  (1): Hardswish()\n",
      "  (2): Dropout(p=0.2, inplace=True)\n",
      "  (3): Linear(in_features=3200, out_features=6, bias=True)\n",
      "  (4): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mobilenetv3_large.classifier = nn.Sequential(\n",
    "                                                nn.Linear(in_features=47040, out_features=10*10*32, bias=True),\n",
    "                                                nn.Hardswish(inplace=True),\n",
    "                                                nn.Dropout(p=.2, inplace=True),\n",
    "                                                nn.Linear(in_features=10*10*32, out_features=6, bias=True),\n",
    "                                                nn.Softmax()\n",
    "                                            )\n",
    "print(mobilenetv3_large.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgpool layer Identity()\n",
      "fc layer Identity()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 47040])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenetv3_large.avgpool = nn.Identity()\n",
    "mobilenetv3_large.classifier = nn.Identity()\n",
    "print('avgpool layer',mobilenetv3_large.avgpool)\n",
    "print('fc layer',mobilenetv3_large.classifier)\n",
    "img = th.rand((1,3,224,224))\n",
    "x = mobilenetv3_large(img)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training Loop**\n",
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model classifier: Sequential(\n",
      "  (0): Linear(in_features=960, out_features=1280, bias=True)\n",
      "  (1): Hardswish()\n",
      "  (2): Dropout(p=0.2, inplace=True)\n",
      "  (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
      ")\n",
      "Modified models classifier: Sequential(\n",
      "  (0): Linear(in_features=960, out_features=1280, bias=True)\n",
      "  (1): Hardswish()\n",
      "  (2): Dropout(p=0.2, inplace=True)\n",
      "  (3): Linear(in_features=1280, out_features=6, bias=True)\n",
      "  (4): Softmax(dim=1)\n",
      ")\n",
      "[INFO] training the network...\n",
      "Start Time: 1714212866.4572513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c20cf38b11b407d9693692b5efd3738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0207b6c6a748e18465949ca96dbb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batch:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0 - Loss: 1.7968 \n",
      "Epoch     0 - Loss: 1.7837 \n",
      "Epoch     0 - Loss: 1.8075 \n",
      "Epoch     0 - Loss: 1.7935 \n",
      "Epoch     0 - Loss: 1.8134 \n",
      "Epoch     0 - Loss: 1.8348 \n",
      "Epoch     0 - Loss: 1.8059 \n",
      "Epoch     0 - Loss: 1.7911 \n",
      "Epoch     0 - Loss: 1.8076 \n",
      "Epoch     0 - Loss: 1.8191 \n",
      "Epoch     0 - Loss: 1.8389 \n",
      "Epoch     0 - Loss: 1.8437 \n",
      "Epoch     0 - Loss: 1.858  \n",
      "Epoch     0 - Loss: 1.8401 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8214 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.9325 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.4325 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.5436 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.8769 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.5991 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.8214 \n",
      "Epoch     0 - Loss: 1.9498 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8769 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.5748 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.5991 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.9498 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.5748 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.5748 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.9325 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.9325 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.9325 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8214 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.9498 \n",
      "Epoch     0 - Loss: 1.5991 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.9498 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.5748 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8214 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8769 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.9498 \n",
      "Epoch     0 - Loss: 1.5991 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.5436 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.8214 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.5748 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.9325 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.9811 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8769 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.4811 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8769 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8214 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.5748 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.9498 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8214 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.5436 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.9498 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8214 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.9498 \n",
      "Epoch     0 - Loss: 1.5436 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.5436 \n",
      "Epoch     0 - Loss: 1.9325 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.8214 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8769 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.488  \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.5748 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8214 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8214 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.9498 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.9811 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.5748 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8769 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8769 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.9498 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6061 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.5991 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.5123 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8769 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8214 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.8769 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.9325 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8769 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.5748 \n",
      "Epoch     0 - Loss: 1.7103 \n",
      "Epoch     0 - Loss: 1.5748 \n",
      "Epoch     0 - Loss: 1.9498 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.8873 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7658 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.5991 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.5748 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.6547 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.8214 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6686 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7936 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8214 \n",
      "Epoch     0 - Loss: 1.7278 \n",
      "Epoch     0 - Loss: 1.7623 \n",
      "Epoch     0 - Loss: 1.9186 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.8561 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.6373 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.8248 \n",
      "Epoch     0 - Loss: 1.6998 \n",
      "Epoch     0 - Loss: 1.7311 \n",
      "Epoch     0 - Loss: 1.7658 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8507c986b3354fc6973e5bb753cc23d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batch:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6547 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8769 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.9498 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7658 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7658 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.5991 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6547 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.5748 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7103 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.5123 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.9186 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7658 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8769 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.9498 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.5991 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.5436 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.5748 \n",
      "Epoch     1 - Loss: 1.988  \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.5748 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6547 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7103 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7658 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.5436 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6547 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8769 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.9325 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.9186 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.5748 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6547 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.5748 \n",
      "Epoch     1 - Loss: 1.9325 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.5991 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8769 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.9811 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.5123 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7658 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.5748 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.988  \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.9498 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7103 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.9186 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8769 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.5436 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.9186 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.5748 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7103 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.9186 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.488  \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7658 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.9325 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.9325 \n",
      "Epoch     1 - Loss: 1.9186 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6547 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.5748 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7103 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.9498 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7658 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7658 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8769 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6547 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.9325 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.9811 \n",
      "Epoch     1 - Loss: 1.5748 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7658 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.9186 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.488  \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.9325 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.5436 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7103 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.8769 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.9498 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6547 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7103 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.5748 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7658 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.9186 \n",
      "Epoch     1 - Loss: 1.5748 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6547 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.5436 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7658 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7103 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7103 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.9186 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7658 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.9186 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7103 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8769 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.5436 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.9186 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7658 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7103 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.5436 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.9186 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.9498 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6547 \n",
      "Epoch     1 - Loss: 1.9498 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6547 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7103 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8769 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 2.0436 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7103 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7103 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.9186 \n",
      "Epoch     1 - Loss: 1.5748 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8769 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.9325 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.5748 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8769 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.8769 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.9186 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6373 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.8214 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6061 \n",
      "Epoch     1 - Loss: 1.8769 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.8873 \n",
      "Epoch     1 - Loss: 1.7623 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7658 \n",
      "Epoch     1 - Loss: 1.7804 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7311 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.6998 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.8248 \n",
      "Epoch     1 - Loss: 1.8561 \n",
      "Epoch     1 - Loss: 1.6686 \n",
      "Epoch     1 - Loss: 1.7936 \n",
      "Epoch     1 - Loss: 1.7658 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f1b9b3a59d47bfb50ef1c3a0eb1472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batch:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8769 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.5436 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.9325 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8769 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.5436 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8769 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.5991 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.9325 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.9325 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.5748 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6547 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.6547 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8769 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.5436 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.9811 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.5748 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.9325 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6547 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.9325 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8872 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.856  \n",
      "Epoch     2 - Loss: 1.6374 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6374 \n",
      "Epoch     2 - Loss: 1.8872 \n",
      "Epoch     2 - Loss: 1.8213 \n",
      "Epoch     2 - Loss: 1.5749 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.5436 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.9325 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.5436 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.5436 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8769 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6547 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6547 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.9498 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6547 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.5436 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.6547 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.5748 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.5748 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.8769 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.5991 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.5436 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.488  \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.5748 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6547 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6547 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.5748 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.9498 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.5748 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.9498 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.5436 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.9498 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.5748 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.5436 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8769 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.5748 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.5436 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.5436 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.9811 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.5748 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.5748 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.9498 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8769 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.5123 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8769 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.5436 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.5991 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.5748 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.5991 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8214 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.5748 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8769 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.5991 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8769 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.9186 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.6373 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7658 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6061 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7311 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8561 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.7103 \n",
      "Epoch     2 - Loss: 1.6225 \n",
      "Epoch     2 - Loss: 1.6686 \n",
      "Epoch     2 - Loss: 1.7623 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.5749 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.7936 \n",
      "Epoch     2 - Loss: 1.6998 \n",
      "Epoch     2 - Loss: 1.8248 \n",
      "Epoch     2 - Loss: 1.6374 \n",
      "Epoch     2 - Loss: 1.8873 \n",
      "Epoch     2 - Loss: 1.7658 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c072bbc4d5d4092a53f62dd235f615a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batch:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6374 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.9185 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.5749 \n",
      "Epoch     3 - Loss: 1.8872 \n",
      "Epoch     3 - Loss: 1.7935 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7658 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.855  \n",
      "Epoch     3 - Loss: 1.7308 \n",
      "Epoch     3 - Loss: 1.7308 \n",
      "Epoch     3 - Loss: 1.6066 \n",
      "Epoch     3 - Loss: 1.6066 \n",
      "Epoch     3 - Loss: 1.8862 \n",
      "Epoch     3 - Loss: 1.8241 \n",
      "Epoch     3 - Loss: 1.8551 \n",
      "Epoch     3 - Loss: 1.6688 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.824  \n",
      "Epoch     3 - Loss: 1.7654 \n",
      "Epoch     3 - Loss: 1.8551 \n",
      "Epoch     3 - Loss: 1.8008 \n",
      "Epoch     3 - Loss: 1.7931 \n",
      "Epoch     3 - Loss: 1.7955 \n",
      "Epoch     3 - Loss: 1.8022 \n",
      "Epoch     3 - Loss: 1.7942 \n",
      "Epoch     3 - Loss: 1.8024 \n",
      "Epoch     3 - Loss: 1.8024 \n",
      "Epoch     3 - Loss: 1.8007 \n",
      "Epoch     3 - Loss: 1.8093 \n",
      "Epoch     3 - Loss: 1.8011 \n",
      "Epoch     3 - Loss: 1.7921 \n",
      "Epoch     3 - Loss: 1.8232 \n",
      "Epoch     3 - Loss: 1.8083 \n",
      "Epoch     3 - Loss: 1.7395 \n",
      "Epoch     3 - Loss: 1.7541 \n",
      "Epoch     3 - Loss: 1.7208 \n",
      "Epoch     3 - Loss: 1.747  \n",
      "Epoch     3 - Loss: 1.7944 \n",
      "Epoch     3 - Loss: 1.8181 \n",
      "Epoch     3 - Loss: 1.6978 \n",
      "Epoch     3 - Loss: 1.7979 \n",
      "Epoch     3 - Loss: 1.7597 \n",
      "Epoch     3 - Loss: 1.7586 \n",
      "Epoch     3 - Loss: 1.7427 \n",
      "Epoch     3 - Loss: 1.7198 \n",
      "Epoch     3 - Loss: 1.7477 \n",
      "Epoch     3 - Loss: 1.5446 \n",
      "Epoch     3 - Loss: 1.7306 \n",
      "Epoch     3 - Loss: 1.7    \n",
      "Epoch     3 - Loss: 1.7308 \n",
      "Epoch     3 - Loss: 1.824  \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.917  \n",
      "Epoch     3 - Loss: 1.793  \n",
      "Epoch     3 - Loss: 1.9171 \n",
      "Epoch     3 - Loss: 1.7928 \n",
      "Epoch     3 - Loss: 1.7621 \n",
      "Epoch     3 - Loss: 1.5443 \n",
      "Epoch     3 - Loss: 1.6687 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.5436 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8214 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6547 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.5748 \n",
      "Epoch     3 - Loss: 1.5436 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8214 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.5436 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.9325 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8214 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8214 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8214 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6547 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.8769 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7658 \n",
      "Epoch     3 - Loss: 1.5748 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6547 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7658 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6547 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.5436 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7658 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.9325 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.8214 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.988  \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.5748 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8769 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.5124 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.5436 \n",
      "Epoch     3 - Loss: 1.8769 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.731  \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8247 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8872 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.6547 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8558 \n",
      "Epoch     3 - Loss: 1.7312 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.856  \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6997 \n",
      "Epoch     3 - Loss: 1.7933 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7933 \n",
      "Epoch     3 - Loss: 1.7622 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6546 \n",
      "Epoch     3 - Loss: 1.6999 \n",
      "Epoch     3 - Loss: 1.7    \n",
      "Epoch     3 - Loss: 1.8558 \n",
      "Epoch     3 - Loss: 1.7932 \n",
      "Epoch     3 - Loss: 1.6997 \n",
      "Epoch     3 - Loss: 1.6376 \n",
      "Epoch     3 - Loss: 1.7934 \n",
      "Epoch     3 - Loss: 1.6376 \n",
      "Epoch     3 - Loss: 1.7309 \n",
      "Epoch     3 - Loss: 1.7619 \n",
      "Epoch     3 - Loss: 1.6997 \n",
      "Epoch     3 - Loss: 1.8553 \n",
      "Epoch     3 - Loss: 1.9869 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8548 \n",
      "Epoch     3 - Loss: 1.824  \n",
      "Epoch     3 - Loss: 1.7926 \n",
      "Epoch     3 - Loss: 1.8239 \n",
      "Epoch     3 - Loss: 1.7931 \n",
      "Epoch     3 - Loss: 1.7    \n",
      "Epoch     3 - Loss: 1.7306 \n",
      "Epoch     3 - Loss: 1.6376 \n",
      "Epoch     3 - Loss: 1.7306 \n",
      "Epoch     3 - Loss: 1.6067 \n",
      "Epoch     3 - Loss: 1.8242 \n",
      "Epoch     3 - Loss: 1.71   \n",
      "Epoch     3 - Loss: 1.8547 \n",
      "Epoch     3 - Loss: 1.789  \n",
      "Epoch     3 - Loss: 1.6099 \n",
      "Epoch     3 - Loss: 1.6705 \n",
      "Epoch     3 - Loss: 1.6971 \n",
      "Epoch     3 - Loss: 1.6687 \n",
      "Epoch     3 - Loss: 1.7018 \n",
      "Epoch     3 - Loss: 1.7293 \n",
      "Epoch     3 - Loss: 1.9405 \n",
      "Epoch     3 - Loss: 1.789  \n",
      "Epoch     3 - Loss: 1.7596 \n",
      "Epoch     3 - Loss: 1.8799 \n",
      "Epoch     3 - Loss: 1.8149 \n",
      "Epoch     3 - Loss: 1.7312 \n",
      "Epoch     3 - Loss: 1.7297 \n",
      "Epoch     3 - Loss: 1.7241 \n",
      "Epoch     3 - Loss: 1.7838 \n",
      "Epoch     3 - Loss: 1.7068 \n",
      "Epoch     3 - Loss: 1.815  \n",
      "Epoch     3 - Loss: 1.7525 \n",
      "Epoch     3 - Loss: 1.7637 \n",
      "Epoch     3 - Loss: 1.8378 \n",
      "Epoch     3 - Loss: 1.7526 \n",
      "Epoch     3 - Loss: 1.6332 \n",
      "Epoch     3 - Loss: 1.6957 \n",
      "Epoch     3 - Loss: 1.8081 \n",
      "Epoch     3 - Loss: 1.7526 \n",
      "Epoch     3 - Loss: 1.769  \n",
      "Epoch     3 - Loss: 1.7995 \n",
      "Epoch     3 - Loss: 1.7687 \n",
      "Epoch     3 - Loss: 1.7577 \n",
      "Epoch     3 - Loss: 1.8499 \n",
      "Epoch     3 - Loss: 1.776  \n",
      "Epoch     3 - Loss: 1.7871 \n",
      "Epoch     3 - Loss: 1.7996 \n",
      "Epoch     3 - Loss: 1.7258 \n",
      "Epoch     3 - Loss: 1.7925 \n",
      "Epoch     3 - Loss: 1.7689 \n",
      "Epoch     3 - Loss: 1.7126 \n",
      "Epoch     3 - Loss: 1.7526 \n",
      "Epoch     3 - Loss: 1.8326 \n",
      "Epoch     3 - Loss: 1.8522 \n",
      "Epoch     3 - Loss: 1.8177 \n",
      "Epoch     3 - Loss: 1.7822 \n",
      "Epoch     3 - Loss: 1.7845 \n",
      "Epoch     3 - Loss: 1.7652 \n",
      "Epoch     3 - Loss: 1.8606 \n",
      "Epoch     3 - Loss: 1.8662 \n",
      "Epoch     3 - Loss: 1.7651 \n",
      "Epoch     3 - Loss: 1.8385 \n",
      "Epoch     3 - Loss: 1.8761 \n",
      "Epoch     3 - Loss: 1.9059 \n",
      "Epoch     3 - Loss: 1.8382 \n",
      "Epoch     3 - Loss: 1.7619 \n",
      "Epoch     3 - Loss: 1.7893 \n",
      "Epoch     3 - Loss: 1.7493 \n",
      "Epoch     3 - Loss: 1.766  \n",
      "Epoch     3 - Loss: 1.7286 \n",
      "Epoch     3 - Loss: 1.7588 \n",
      "Epoch     3 - Loss: 1.7599 \n",
      "Epoch     3 - Loss: 1.7536 \n",
      "Epoch     3 - Loss: 1.7328 \n",
      "Epoch     3 - Loss: 1.8056 \n",
      "Epoch     3 - Loss: 1.7756 \n",
      "Epoch     3 - Loss: 1.6721 \n",
      "Epoch     3 - Loss: 1.7388 \n",
      "Epoch     3 - Loss: 1.7397 \n",
      "Epoch     3 - Loss: 1.8243 \n",
      "Epoch     3 - Loss: 1.689  \n",
      "Epoch     3 - Loss: 1.783  \n",
      "Epoch     3 - Loss: 1.7472 \n",
      "Epoch     3 - Loss: 1.6084 \n",
      "Epoch     3 - Loss: 1.7106 \n",
      "Epoch     3 - Loss: 1.7561 \n",
      "Epoch     3 - Loss: 1.8213 \n",
      "Epoch     3 - Loss: 1.7739 \n",
      "Epoch     3 - Loss: 1.7821 \n",
      "Epoch     3 - Loss: 1.7144 \n",
      "Epoch     3 - Loss: 1.7406 \n",
      "Epoch     3 - Loss: 1.6961 \n",
      "Epoch     3 - Loss: 1.818  \n",
      "Epoch     3 - Loss: 1.756  \n",
      "Epoch     3 - Loss: 1.7559 \n",
      "Epoch     3 - Loss: 1.6147 \n",
      "Epoch     3 - Loss: 1.6961 \n",
      "Epoch     3 - Loss: 1.8134 \n",
      "Epoch     3 - Loss: 1.7798 \n",
      "Epoch     3 - Loss: 1.7871 \n",
      "Epoch     3 - Loss: 1.6985 \n",
      "Epoch     3 - Loss: 1.811  \n",
      "Epoch     3 - Loss: 1.7625 \n",
      "Epoch     3 - Loss: 1.7918 \n",
      "Epoch     3 - Loss: 1.8543 \n",
      "Epoch     3 - Loss: 1.7923 \n",
      "Epoch     3 - Loss: 1.5763 \n",
      "Epoch     3 - Loss: 1.9158 \n",
      "Epoch     3 - Loss: 1.6996 \n",
      "Epoch     3 - Loss: 1.7001 \n",
      "Epoch     3 - Loss: 1.6996 \n",
      "Epoch     3 - Loss: 1.6691 \n",
      "Epoch     3 - Loss: 1.7926 \n",
      "Epoch     3 - Loss: 1.7308 \n",
      "Epoch     3 - Loss: 1.7613 \n",
      "Epoch     3 - Loss: 1.9294 \n",
      "Epoch     3 - Loss: 1.8858 \n",
      "Epoch     3 - Loss: 1.7934 \n",
      "Epoch     3 - Loss: 1.8558 \n",
      "Epoch     3 - Loss: 1.7934 \n",
      "Epoch     3 - Loss: 1.6062 \n",
      "Epoch     3 - Loss: 1.731  \n",
      "Epoch     3 - Loss: 1.8247 \n",
      "Epoch     3 - Loss: 1.8245 \n",
      "Epoch     3 - Loss: 1.731  \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.731  \n",
      "Epoch     3 - Loss: 1.731  \n",
      "Epoch     3 - Loss: 1.7101 \n",
      "Epoch     3 - Loss: 1.8246 \n",
      "Epoch     3 - Loss: 1.7622 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.4812 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8872 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8559 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7622 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7622 \n",
      "Epoch     3 - Loss: 1.7657 \n",
      "Epoch     3 - Loss: 1.6374 \n",
      "Epoch     3 - Loss: 1.856  \n",
      "Epoch     3 - Loss: 1.6062 \n",
      "Epoch     3 - Loss: 1.7935 \n",
      "Epoch     3 - Loss: 1.856  \n",
      "Epoch     3 - Loss: 1.8247 \n",
      "Epoch     3 - Loss: 1.6374 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6999 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.9185 \n",
      "Epoch     3 - Loss: 1.6999 \n",
      "Epoch     3 - Loss: 1.8213 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7935 \n",
      "Epoch     3 - Loss: 1.6374 \n",
      "Epoch     3 - Loss: 1.7935 \n",
      "Epoch     3 - Loss: 1.6374 \n",
      "Epoch     3 - Loss: 1.7935 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8872 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7935 \n",
      "Epoch     3 - Loss: 1.6374 \n",
      "Epoch     3 - Loss: 1.8872 \n",
      "Epoch     3 - Loss: 1.6547 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.731  \n",
      "Epoch     3 - Loss: 1.7935 \n",
      "Epoch     3 - Loss: 1.7935 \n",
      "Epoch     3 - Loss: 1.7622 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.7935 \n",
      "Epoch     3 - Loss: 1.8247 \n",
      "Epoch     3 - Loss: 1.7622 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6547 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.8246 \n",
      "Epoch     3 - Loss: 1.6376 \n",
      "Epoch     3 - Loss: 1.7933 \n",
      "Epoch     3 - Loss: 1.8557 \n",
      "Epoch     3 - Loss: 1.8868 \n",
      "Epoch     3 - Loss: 1.6375 \n",
      "Epoch     3 - Loss: 1.6062 \n",
      "Epoch     3 - Loss: 1.7309 \n",
      "Epoch     3 - Loss: 1.6685 \n",
      "Epoch     3 - Loss: 1.8244 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8766 \n",
      "Epoch     3 - Loss: 1.7624 \n",
      "Epoch     3 - Loss: 1.7933 \n",
      "Epoch     3 - Loss: 1.824  \n",
      "Epoch     3 - Loss: 1.7929 \n",
      "Epoch     3 - Loss: 1.6066 \n",
      "Epoch     3 - Loss: 1.6372 \n",
      "Epoch     3 - Loss: 1.7929 \n",
      "Epoch     3 - Loss: 1.6687 \n",
      "Epoch     3 - Loss: 1.7619 \n",
      "Epoch     3 - Loss: 1.6996 \n",
      "Epoch     3 - Loss: 1.886  \n",
      "Epoch     3 - Loss: 1.7934 \n",
      "Epoch     3 - Loss: 1.8209 \n",
      "Epoch     3 - Loss: 1.669  \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7622 \n",
      "Epoch     3 - Loss: 1.6687 \n",
      "Epoch     3 - Loss: 1.9177 \n",
      "Epoch     3 - Loss: 1.6996 \n",
      "Epoch     3 - Loss: 1.7309 \n",
      "Epoch     3 - Loss: 1.7308 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7934 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8867 \n",
      "Epoch     3 - Loss: 1.655  \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.6687 \n",
      "Epoch     3 - Loss: 1.6063 \n",
      "Epoch     3 - Loss: 1.8246 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.762  \n",
      "Epoch     3 - Loss: 1.7308 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7622 \n",
      "Epoch     3 - Loss: 1.9181 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7933 \n",
      "Epoch     3 - Loss: 1.8211 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7934 \n",
      "Epoch     3 - Loss: 1.731  \n",
      "Epoch     3 - Loss: 1.8247 \n",
      "Epoch     3 - Loss: 1.887  \n",
      "Epoch     3 - Loss: 1.731  \n",
      "Epoch     3 - Loss: 1.8246 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.731  \n",
      "Epoch     3 - Loss: 1.7934 \n",
      "Epoch     3 - Loss: 1.6687 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7657 \n",
      "Epoch     3 - Loss: 1.9182 \n",
      "Epoch     3 - Loss: 1.7309 \n",
      "Epoch     3 - Loss: 1.6684 \n",
      "Epoch     3 - Loss: 1.6379 \n",
      "Epoch     3 - Loss: 1.7931 \n",
      "Epoch     3 - Loss: 1.7308 \n",
      "Epoch     3 - Loss: 1.8867 \n",
      "Epoch     3 - Loss: 1.8241 \n",
      "Epoch     3 - Loss: 1.8867 \n",
      "Epoch     3 - Loss: 1.7927 \n",
      "Epoch     3 - Loss: 1.7617 \n",
      "Epoch     3 - Loss: 1.6685 \n",
      "Epoch     3 - Loss: 1.5994 \n",
      "Epoch     3 - Loss: 1.7621 \n",
      "Epoch     3 - Loss: 1.6382 \n",
      "Epoch     3 - Loss: 1.6991 \n",
      "Epoch     3 - Loss: 1.8225 \n",
      "Epoch     3 - Loss: 1.7884 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7898 \n",
      "Epoch     3 - Loss: 1.7586 \n",
      "Epoch     3 - Loss: 1.7296 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6991 \n",
      "Epoch     3 - Loss: 1.8225 \n",
      "Epoch     3 - Loss: 1.9245 \n",
      "Epoch     3 - Loss: 1.8545 \n",
      "Epoch     3 - Loss: 1.9172 \n",
      "Epoch     3 - Loss: 1.8009 \n",
      "Epoch     3 - Loss: 1.6957 \n",
      "Epoch     3 - Loss: 1.8437 \n",
      "Epoch     3 - Loss: 1.9345 \n",
      "Epoch     3 - Loss: 1.7953 \n",
      "Epoch     3 - Loss: 1.8777 \n",
      "Epoch     3 - Loss: 1.9455 \n",
      "Epoch     3 - Loss: 1.8999 \n",
      "Epoch     3 - Loss: 1.8917 \n",
      "Epoch     3 - Loss: 1.8065 \n",
      "Epoch     3 - Loss: 1.8792 \n",
      "Epoch     3 - Loss: 1.8804 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8836 \n",
      "Epoch     3 - Loss: 1.9446 \n",
      "Epoch     3 - Loss: 1.914  \n",
      "Epoch     3 - Loss: 1.9139 \n",
      "Epoch     3 - Loss: 1.8525 \n",
      "Epoch     3 - Loss: 1.8835 \n",
      "Epoch     3 - Loss: 1.8831 \n",
      "Epoch     3 - Loss: 1.8223 \n",
      "Epoch     3 - Loss: 1.8527 \n",
      "Epoch     3 - Loss: 1.9752 \n",
      "Epoch     3 - Loss: 1.7917 \n",
      "Epoch     3 - Loss: 1.8732 \n",
      "Epoch     3 - Loss: 1.8528 \n",
      "Epoch     3 - Loss: 1.8877 \n",
      "Epoch     3 - Loss: 1.8894 \n",
      "Epoch     3 - Loss: 1.8626 \n",
      "Epoch     3 - Loss: 1.8339 \n",
      "Epoch     3 - Loss: 1.9144 \n",
      "Epoch     3 - Loss: 1.8338 \n",
      "Epoch     3 - Loss: 1.859  \n",
      "Epoch     3 - Loss: 1.8897 \n",
      "Epoch     3 - Loss: 1.806  \n",
      "Epoch     3 - Loss: 1.8599 \n",
      "Epoch     3 - Loss: 1.8358 \n",
      "Epoch     3 - Loss: 1.8511 \n",
      "Epoch     3 - Loss: 1.8069 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.8214 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.5748 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8769 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7658 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7658 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7658 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8214 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.5436 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7658 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7658 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8769 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.9325 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 2.0123 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8214 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 1.8214 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 2.0436 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.9325 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.988  \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 2.0436 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.9325 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.9325 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7658 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.9498 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8769 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.9811 \n",
      "Epoch     3 - Loss: 2.0123 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8214 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.9185 \n",
      "Epoch     3 - Loss: 1.856  \n",
      "Epoch     3 - Loss: 1.981  \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.9185 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.9185 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.9324 \n",
      "Epoch     3 - Loss: 1.9185 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7658 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.5436 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8561 \n",
      "Epoch     3 - Loss: 1.5436 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8214 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.8873 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.9186 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7103 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.6061 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8214 \n",
      "Epoch     3 - Loss: 1.8331 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6998 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.8248 \n",
      "Epoch     3 - Loss: 1.6373 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7936 \n",
      "Epoch     3 - Loss: 1.6686 \n",
      "Epoch     3 - Loss: 1.7623 \n",
      "Epoch     3 - Loss: 1.7311 \n",
      "Epoch     3 - Loss: 1.7658 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3aed99386c544f0985b091a1e82cacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batch:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.5748 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6547 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8769 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.5436 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.5436 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.5748 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.5748 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.5991 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6547 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.5748 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.9498 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.9325 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.988  \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.5436 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6547 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.5436 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.5748 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.9325 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.9498 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6547 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.9498 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6547 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.8769 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.9811 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.488  \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.5436 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.5991 \n",
      "Epoch     4 - Loss: 1.9498 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.4811 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6547 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8769 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.9498 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8769 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.5748 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.5991 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.5748 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.5748 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.5748 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.5991 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6547 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8769 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8769 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8769 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.9325 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.5436 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.8769 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8769 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8769 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.5436 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.5748 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.9498 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.5748 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8769 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.9325 \n",
      "Epoch     4 - Loss: 1.5748 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.9498 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.9811 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6547 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.5991 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.9498 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.6547 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.4186 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.9498 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6547 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6547 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6547 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.9498 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.5436 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.5436 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.9186 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.5436 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.8214 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.7658 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.5436 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.5991 \n",
      "Epoch     4 - Loss: 1.6061 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.9325 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.8769 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.8561 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7103 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.5991 \n",
      "Epoch     4 - Loss: 1.7936 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8873 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.6547 \n",
      "Epoch     4 - Loss: 1.7278 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6998 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.7311 \n",
      "Epoch     4 - Loss: 1.7623 \n",
      "Epoch     4 - Loss: 1.8248 \n",
      "Epoch     4 - Loss: 1.6686 \n",
      "Epoch     4 - Loss: 1.6373 \n",
      "Epoch     4 - Loss: 1.8769 \n",
      "[INFO] total time taken to train the model: 7528.36s\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import torchvision as tv\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import wandb\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "lr = 7e-3\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# Models\n",
    "mobilenetv3_large = tv.models.mobilenet_v3_large(weights='IMAGENET1K_V1')\n",
    "print('Original model classifier:', mobilenetv3_large.classifier)\n",
    "mobilenetv3_large.classifier = nn.Sequential(\n",
    "                                                nn.Linear(in_features=960, out_features=1280, bias=True),\n",
    "                                                nn.Hardswish(inplace=True),\n",
    "                                                nn.Dropout(p=.2, inplace=True),\n",
    "                                                nn.Linear(in_features=1280, out_features=6, bias=True),\n",
    "                                                nn.Softmax(dim=1)\n",
    "                                            )\n",
    "print('Modified models classifier:', mobilenetv3_large.classifier)\n",
    "\n",
    "# Criterion\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = th.optim.Adam(lr=lr, params=mobilenetv3_large.parameters(), weight_decay=1e-4)\n",
    "\n",
    "########################################\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "print('Start Time:', startTime)\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\", position=1):\n",
    "  \n",
    "  # get a new batch\n",
    "  epoch_images = []\n",
    "  train_losses = []\n",
    "\n",
    "  for batch, labels in (pbar:= tqdm(train_loader, desc=\"batch\", position=0)):\n",
    "    # batch = batch.to(device)\n",
    "    # labels = labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True) # optimizer.zero_grad()\n",
    "    transformed = mobilenetv3_large.forward(batch)\n",
    "\n",
    "    # Loss calculation\n",
    "    loss = loss_fn(transformed, labels)\n",
    "\n",
    "    # Metrics calculation\n",
    "    # ADD METRIC\n",
    "    \n",
    "    # Backpropagation step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss)\n",
    "\n",
    "    print(f\"Epoch {1+epoch:5} - Loss: {loss.item():<7.5}\")  # print(f\"Epoch {1+epoch:5} - Loss: {loss.item():<7.5}\")\n",
    "    \n",
    "    mobilenetv3_large.eval()\n",
    "    with th.no_grad():\n",
    "       val_losses = []\n",
    "       result_images = []\n",
    "       \n",
    "       for batch, labels in val_loader:\n",
    "         transformed = mobilenetv3_large.forward(batch)\n",
    "\n",
    "         val_loss = loss_fn(transformed, labels) # change: labels:long to labels.type('torch.FloatTensor').to(device)\n",
    "         val_losses.append(val_loss.item())\n",
    "         print(f\"Epoch {1+epoch:5} - Loss: {val_loss.item():<7.5}\")\n",
    "\n",
    "endTime = time.time()\n",
    "endTime = endTime\n",
    "running_time = endTime - startTime\n",
    "running_time = running_time\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(running_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model classifier: Sequential(\n",
      "  (0): Linear(in_features=960, out_features=1280, bias=True)\n",
      "  (1): Hardswish()\n",
      "  (2): Dropout(p=0.2, inplace=True)\n",
      "  (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
      ")\n",
      "Modified models classifier: Sequential(\n",
      "  (0): Linear(in_features=960, out_features=1280, bias=True)\n",
      "  (1): Hardswish()\n",
      "  (2): Dropout(p=0.2, inplace=True)\n",
      "  (3): Linear(in_features=1280, out_features=6, bias=True)\n",
      "  (4): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--learning_rate LEARNING_RATE]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\BenaventeD\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-119242l0nu7OZ5HhJ.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import wandb\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "from Dataloader.Dataloader import train_loader, val_loader\n",
    "\n",
    "#######################\n",
    "# Hyper-Parameters\n",
    "lr = 3e-4\n",
    "batch_size = 10\n",
    "optimizer = \"adam\"\n",
    "epochs=100\n",
    "\n",
    "# Models\n",
    "mobilenetv3_large = tv.models.mobilenet_v3_large(weights='IMAGENET1K_V1')\n",
    "print('Original model classifier:', mobilenetv3_large.classifier)\n",
    "mobilenetv3_large.classifier = nn.Sequential(\n",
    "                                                nn.Linear(in_features=960, out_features=1280, bias=True),\n",
    "                                                nn.Hardswish(inplace=True),\n",
    "                                                nn.Dropout(p=.2, inplace=True),\n",
    "                                                nn.Linear(in_features=1280, out_features=6, bias=True),\n",
    "                                                nn.Softmax(dim=1)\n",
    "                                            )\n",
    "print('Modified models classifier:', mobilenetv3_large.classifier)\n",
    "\n",
    "# Criterion\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = th.optim.Adam(lr=lr, params=mobilenetv3_large.parameters(), weight_decay=1e-4)\n",
    "\n",
    "##### Hyperparameter search #####\n",
    "#################################\n",
    "parser = argparse.ArgumentParser(\n",
    "                    # prog='Traning CNNs',\n",
    "                    # description='Train the CNNs and Hyperparameter searching',\n",
    "                    # epilog='Current model MobileNetV3_Large'\n",
    "                    )\n",
    "\n",
    "parser.add_argument('--learning_rate', type=float, default=lr)\n",
    "# parser.add_argument('--optimizer', type=str, default=optimizer)\n",
    "parser.add_argument('--batch_size', type=int, default=batch_size)\n",
    "# parser.add_argument('--model', type=str, default=model)\n",
    "# parser.add_argument('--cc_loss_weight', type=float, default=cc_loss_weight)\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "lr = args.learning_rate\n",
    "# optimizer = args.optimizer\n",
    "batch_size = args.batch_size\n",
    "# model = args.model\n",
    "# cc_loss_weight = args.cc_loss_weight\n",
    "\n",
    "#################################\n",
    "\n",
    "#######################################\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"project s-mad\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "          \"learning_rate\": lr,\n",
    "          \"epochs\": epochs,\n",
    "          \"start_time\": startTime,\n",
    "    }\n",
    ")\n",
    "\n",
    "# best_model_state = None\n",
    "# best_metric_value = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\", position=1):\n",
    "  \n",
    "  # get a new batch\n",
    "  epoch_images = []\n",
    "  train_losses = []\n",
    "\n",
    "  for batch, labels in (pbar:= tqdm(train_loader, desc=\"batch\", position=0)):\n",
    "    # batch = batch.to(device)\n",
    "    # labels = labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True) # optimizer.zero_grad()\n",
    "    transformed = mobilenetv3_large.forward(batch)\n",
    "\n",
    "    # Loss calculation\n",
    "    loss = loss_fn(transformed, labels)\n",
    "\n",
    "    # Metrics calculation\n",
    "    # ADD METRIC\n",
    "    \n",
    "    # Backpropagation step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss)\n",
    "\n",
    "    print(f\"Epoch {1+epoch:5} - Loss: {loss.item():<7.5}\")  # print(f\"Epoch {1+epoch:5} - Loss: {loss.item():<7.5}\")\n",
    "    \n",
    "    mobilenetv3_large.eval()\n",
    "    with th.no_grad():\n",
    "       val_losses = []\n",
    "       result_images = []\n",
    "       \n",
    "       for batch, labels in val_loader:\n",
    "         transformed = mobilenetv3_large.forward(batch)\n",
    "\n",
    "         val_loss = loss_fn(transformed, labels) # change: labels:long to labels.type('torch.FloatTensor').to(device)\n",
    "         val_losses.append(val_loss.item())\n",
    "         print(f\"Epoch {1+epoch:5} - Loss: {val_loss.item():<7.5}\")\n",
    "\n",
    "    # Metric calculation\n",
    "    # ADD METRIC\n",
    "  \n",
    "    wandb.log({\n",
    "    \"loss\": np.mean(train_losses),\n",
    "    # \"Acc\": np.mean(train_acc),\n",
    "    # \"lr\": lr,\n",
    "    \"val_loss\": np.mean(val_losses),\n",
    "    # \"val_Acc\": np.mean(test_accs),\n",
    "    })\n",
    "\n",
    "endTime = time.time()\n",
    "endTime = endTime\n",
    "running_time = endTime - startTime\n",
    "running_time = running_time\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))\n",
    "\n",
    "\n",
    "# wandb.log({\n",
    "#     \"end_time\": endTime,\n",
    "#     \"running_time\": running_time\n",
    "#     })\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573793.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test = []\n",
    "\n",
    "for idx in range(10):\n",
    "    random = np.random.randint(0,high=10e5)\n",
    "    test.append(random)\n",
    "    # break\n",
    "\n",
    "print(np.mean(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_smad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
